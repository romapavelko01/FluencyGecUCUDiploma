{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPV/dswt8Bmyg4kCbm9e0C3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romapavelko01/FluencyGecUCUDiploma/blob/main/val_gector_roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This notebook aims to run the errant alignment evaluation script from the unlp-2023 shared task on the predictions made by GECToR's different configurations with RoBERTa "
      ],
      "metadata": {
        "id": "hzlc9yelfo52"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCnyCKUjfiL-",
        "outputId": "2d663ffd-9a2c-45d8-b9c1-7ed361df613f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python3 (python3) in auto mode\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python-pip-whl python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python-pip-whl python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 4 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 2,389 kB of archives.\n",
            "After this operation, 4,933 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python-pip-whl all 20.0.2-5ubuntu1.8 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 python3-setuptools all 45.2.0-1ubuntu0.1 [330 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-wheel all 0.34.2-1ubuntu0.1 [23.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-pip all 20.0.2-5ubuntu1.8 [231 kB]\n",
            "Fetched 2,389 kB in 2s (1,469 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python-pip-whl.\n",
            "(Reading database ... 122531 files and directories currently installed.)\n",
            "Preparing to unpack .../python-pip-whl_20.0.2-5ubuntu1.8_all.deb ...\n",
            "Unpacking python-pip-whl (20.0.2-5ubuntu1.8) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../python3-setuptools_45.2.0-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.34.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_20.0.2-5ubuntu1.8_all.deb ...\n",
            "Unpacking python3-pip (20.0.2-5ubuntu1.8) ...\n",
            "Setting up python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Setting up python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Setting up python-pip-whl (20.0.2-5ubuntu1.8) ...\n",
            "Setting up python3-pip (20.0.2-5ubuntu1.8) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-distutils is already the newest version (3.8.10-0ubuntu1~20.04).\n",
            "python3-distutils set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-apt is already the newest version (2.0.1ubuntu0.20.04.1).\n",
            "python3-apt set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "0 upgraded, 0 newly installed, 1 reinstalled, 0 to remove and 24 not upgraded.\n",
            "Need to get 141 kB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 python3-distutils all 3.8.10-0ubuntu1~20.04 [141 kB]\n",
            "Fetched 141 kB in 1s (183 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 122895 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-distutils_3.8.10-0ubuntu1~20.04_all.deb ...\n",
            "Unpacking python3-distutils (3.8.10-0ubuntu1~20.04) over (3.8.10-0ubuntu1~20.04) ...\n",
            "Setting up python3-distutils (3.8.10-0ubuntu1~20.04) ...\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,049 kB]\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,175 kB]\n",
            "Hit:13 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,699 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2,240 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,378 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,344 kB]\n",
            "Fetched 13.2 MB in 3s (4,003 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "# it is important to put the environment in the 3.8 version of Python so that there will be no errors regarding dependencies\n",
        "def change_to3_8():\n",
        "  !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 4\n",
        "  !sudo apt install python3-pip\n",
        "  !sudo apt-get install python3-distutils\n",
        "  !sudo apt-get install python3-apt\n",
        "  !sudo apt-get install --reinstall python3-distutils\n",
        "  !sudo apt-get update\n",
        "\n",
        "change_to3_8()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/asivokon/unlp-2023-shared-task.git\n",
        "!pip install -r unlp-2023-shared-task/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VlSpKOv4Bvo_",
        "outputId": "90625e9d-6db0-463e-d700-6d97365f0895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'unlp-2023-shared-task'...\n",
            "remote: Enumerating objects: 125, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 125 (delta 76), reused 98 (delta 62), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (125/125), 4.69 MiB | 9.18 MiB/s, done.\n",
            "Resolving deltas: 100% (76/76), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ua-gec==2.0.0\n",
            "  Downloading ua_gec-2.0.0-py3-none-any.whl (6.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.1 MB 7.9 MB/s \n",
            "\u001b[?25hCollecting errant==2.3.3\n",
            "  Downloading errant-2.3.3-py3-none-any.whl (499 kB)\n",
            "\u001b[K     |████████████████████████████████| 499 kB 54.0 MB/s \n",
            "\u001b[?25hCollecting spacy<3,>=2.2.0\n",
            "  Downloading spacy-2.3.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.0 MB 58.1 MB/s \n",
            "\u001b[?25hCollecting stanza==1.4.2\n",
            "  Downloading stanza-1.4.2-py3-none-any.whl (691 kB)\n",
            "\u001b[K     |████████████████████████████████| 691 kB 57.8 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz>=2.0.0\n",
            "  Downloading rapidfuzz-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 59.2 MB/s \n",
            "\u001b[?25hCollecting srsly<1.1.0,>=1.0.2\n",
            "  Downloading srsly-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[K     |████████████████████████████████| 211 kB 71.9 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.15.0\n",
            "  Downloading numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 47.5 MB/s \n",
            "\u001b[?25hCollecting blis<0.8.0,>=0.4.0\n",
            "  Downloading blis-0.7.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2 MB 46.1 MB/s \n",
            "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.8-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 53.2 MB/s \n",
            "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Collecting wasabi<1.1.0,>=0.4.0\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Collecting requests<3.0.0,>=2.13.0\n",
            "  Downloading requests-2.30.0-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3,>=2.2.0->-r unlp-2023-shared-task/requirements.txt (line 3)) (45.2.0)\n",
            "Collecting thinc<7.5.0,>=7.4.1\n",
            "  Downloading thinc-7.4.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 57.5 MB/s \n",
            "\u001b[?25hCollecting tqdm<5.0.0,>=4.38.0\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting protobuf\n",
            "  Downloading protobuf-4.23.1-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[K     |████████████████████████████████| 304 kB 63.3 MB/s \n",
            "\u001b[?25hCollecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting torch>=1.3.0\n",
            "  Downloading torch-2.0.1-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 619.9 MB 14 kB/s \n",
            "\u001b[?25hCollecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[K     |████████████████████████████████| 240 kB 76.6 MB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 66.3 MB/s \n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 53.0 MB/s \n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 95 kB/s \n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 65.7 MB/s \n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 173.2 MB 26 kB/s \n",
            "\u001b[?25hCollecting networkx\n",
            "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 63.3 MB/s \n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 317.1 MB 28 kB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 557.1 MB 7.1 kB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 65 kB/s \n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 177.1 MB 41 kB/s \n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 102.6 MB 2.2 kB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[K     |████████████████████████████████| 849 kB 8.9 MB/s \n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 64.1 MB/s \n",
            "\u001b[?25hCollecting sympy\n",
            "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 44.1 MB/s \n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.6 MB 84 kB/s \n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
            "Collecting triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 63.2 MB 3.6 kB/s \n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 168.4 MB 44 kB/s \n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=1.3.0->stanza==1.4.2->-r unlp-2023-shared-task/requirements.txt (line 4)) (0.34.2)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting mpmath>=0.19\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[K     |████████████████████████████████| 536 kB 77.7 MB/s \n",
            "\u001b[?25hCollecting cmake\n",
            "  Downloading cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.0 MB 108 kB/s \n",
            "\u001b[?25hCollecting lit\n",
            "  Downloading lit-16.0.5.tar.gz (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 65.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji, lit\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234905 sha256=4b91a488189ccd95cd231fba1f5296c5a5cbbf80f10872dcba6f5bbf42912af7\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/62/9e/a6b27a681abcde69970dbc0326ff51955f3beac72f15696984\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-16.0.5-py3-none-any.whl size=88191 sha256=fa5ad3ae160ee3d58b5e8043acbd07407bdc6af126b9319656559dfe692ebbe7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/05/82/bbf80d224f6ad6c764404f26aba7b644bfa77b45dcc1c17f5a\n",
            "Successfully built emoji lit\n",
            "Installing collected packages: ua-gec, rapidfuzz, srsly, numpy, blis, cymem, murmurhash, preshed, wasabi, certifi, urllib3, idna, charset-normalizer, requests, plac, catalogue, tqdm, thinc, spacy, errant, protobuf, six, nvidia-cusparse-cu11, networkx, nvidia-cublas-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cudnn-cu11, nvidia-cuda-cupti-cu11, nvidia-nccl-cu11, nvidia-nvtx-cu11, nvidia-cusolver-cu11, nvidia-cuda-runtime-cu11, MarkupSafe, jinja2, mpmath, sympy, nvidia-curand-cu11, filelock, cmake, lit, triton, nvidia-cufft-cu11, typing-extensions, torch, emoji, stanza\n",
            "Successfully installed MarkupSafe-2.1.2 blis-0.7.9 catalogue-1.0.2 certifi-2023.5.7 charset-normalizer-3.1.0 cmake-3.26.3 cymem-2.0.7 emoji-2.2.0 errant-2.3.3 filelock-3.12.0 idna-3.4 jinja2-3.1.2 lit-16.0.5 mpmath-1.3.0 murmurhash-1.0.9 networkx-3.1 numpy-1.24.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 plac-1.1.3 preshed-3.0.8 protobuf-4.23.1 rapidfuzz-3.0.0 requests-2.30.0 six-1.16.0 spacy-2.3.9 srsly-1.0.6 stanza-1.4.2 sympy-1.12 thinc-7.4.6 torch-2.0.1 tqdm-4.65.0 triton-2.0.0 typing-extensions-4.5.0 ua-gec-2.0.0 urllib3-2.0.2 wasabi-0.10.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG-jc0w2B6lr",
        "outputId": "2d05b7b7-a3a4-4e21-e764-429cd661315f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tn_prob is set to 1 so that model can learn on data with no errors, thus no need to edit"
      ],
      "metadata": {
        "id": "ylHWmzD2J0Qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ac=0, mep=0"
      ],
      "metadata": {
        "id": "IJSxdw6Kim5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation already exists\n",
        "!python unlp-2023-shared-task/scripts/evaluate.py drive/MyDrive/UNLP/predictions/111_ukr_roberta_base/prediction_tn_1_cs_4_ac_mp_00.txt --m2 unlp-2023-shared-task/data/gec-fluency/valid.m2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XInIlL4hJ4Uy",
        "outputId": "16899510-fe41-419d-da5d-c9271497fe52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing submission...\n",
            "2023-05-15 17:33:11 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 94.5MB/s]        \n",
            "2023-05-15 17:33:11 WARNING: Language uk package default expects mwt, which has been added\n",
            "2023-05-15 17:33:11 INFO: Loading these models for language: uk (Ukrainian):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | iu      |\n",
            "| mwt       | iu      |\n",
            "=======================\n",
            "\n",
            "2023-05-15 17:33:11 INFO: Use device: cpu\n",
            "2023-05-15 17:33:11 INFO: Loading: tokenize\n",
            "2023-05-15 17:33:12 INFO: Loading: mwt\n",
            "2023-05-15 17:33:12 INFO: Done loading processors!\n",
            "Tokenized: /tmp/unlp.target.tok\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "Aligned submission: /tmp/unlp.target.m2\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "622\t369\t1135\t0.6276\t0.354\t0.5436\n",
            "==============================================\n",
            "\n",
            "\n",
            "============ Span-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "707\t284\t1120\t0.7134\t0.387\t0.6104\n",
            "==============================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ac=0.2, mep=0.5"
      ],
      "metadata": {
        "id": "B59xT9hxiv-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python unlp-2023-shared-task/scripts/evaluate.py drive/MyDrive/UNLP/predictions/111_ukr_roberta_base/prediction_tn_1_cs_4_ac_02_mp_05.txt --m2 unlp-2023-shared-task/data/gec-fluency/valid.m2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOFi4gLRixbt",
        "outputId": "ce26e957-7649-4fe3-b519-5792fe83df6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading spacy resources...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_sm==2.3.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from en_core_web_sm==2.3.1) (2.3.9)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.30.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (45.2.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.7)\n",
            "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.2)\n",
            "Building wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047105 sha256=0769f3e8a7faf64faf7640679b4a2d689b1e7392d5c2184706208acbf5fe41b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/4d/f7/563214122be1540b5f9197b52cb3ddb9c4a8070808b22d5a84\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-2.3.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.8/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Tokenizing submission...\n",
            "2023-05-19 15:58:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 67.9MB/s]        \n",
            "2023-05-19 15:58:56 WARNING: Language uk package default expects mwt, which has been added\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-uk/resolve/v1.4.1/models/tokenize/iu.pt: 100% 646k/646k [00:00<00:00, 2.58MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-uk/resolve/v1.4.1/models/mwt/iu.pt: 100% 552k/552k [00:00<00:00, 2.20MB/s]\n",
            "2023-05-19 15:58:57 INFO: Loading these models for language: uk (Ukrainian):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | iu      |\n",
            "| mwt       | iu      |\n",
            "=======================\n",
            "\n",
            "2023-05-19 15:58:57 INFO: Use device: cpu\n",
            "2023-05-19 15:58:57 INFO: Loading: tokenize\n",
            "2023-05-19 15:58:58 INFO: Loading: mwt\n",
            "2023-05-19 15:58:58 INFO: Done loading processors!\n",
            "Tokenized: /tmp/unlp.target.tok\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "Aligned submission: /tmp/unlp.target.m2\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "468\t207\t1203\t0.6933\t0.2801\t0.5353\n",
            "==============================================\n",
            "\n",
            "\n",
            "============ Span-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "519\t156\t1186\t0.7689\t0.3044\t0.5891\n",
            "==============================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ac=0.2, mep=0.2"
      ],
      "metadata": {
        "id": "4qO19EvAyysY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python unlp-2023-shared-task/scripts/evaluate.py drive/MyDrive/UNLP/predictions/111_ukr_roberta_base/prediction_tn_1_cs_4_ac_02_mp_02.txt --m2 unlp-2023-shared-task/data/gec-fluency/valid.m2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G35DhnDeyz9x",
        "outputId": "be32d6b8-8eca-4c6a-82a2-3cef4bac0ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing submission...\n",
            "2023-05-19 16:15:39 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 88.4MB/s]        \n",
            "2023-05-19 16:15:39 WARNING: Language uk package default expects mwt, which has been added\n",
            "2023-05-19 16:15:39 INFO: Loading these models for language: uk (Ukrainian):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | iu      |\n",
            "| mwt       | iu      |\n",
            "=======================\n",
            "\n",
            "2023-05-19 16:15:39 INFO: Use device: cpu\n",
            "2023-05-19 16:15:39 INFO: Loading: tokenize\n",
            "2023-05-19 16:15:39 INFO: Loading: mwt\n",
            "2023-05-19 16:15:39 INFO: Done loading processors!\n",
            "Tokenized: /tmp/unlp.target.tok\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "Aligned submission: /tmp/unlp.target.m2\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "544\t267\t1156\t0.6708\t0.32\t0.5502\n",
            "==============================================\n",
            "\n",
            "\n",
            "============ Span-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "602\t209\t1135\t0.7423\t0.3466\t0.6043\n",
            "==============================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Default hyperparameters, --tn_prob=0.0, cold_steps=4"
      ],
      "metadata": {
        "id": "Gtj0ZHyBJ5IV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ac=0, mep=0"
      ],
      "metadata": {
        "id": "9b56Yy-IjJVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation already exists\n",
        "!python unlp-2023-shared-task/scripts/evaluate.py drive/MyDrive/UNLP/predictions/111_ukr_roberta_base/prediction_tn_00_cs_4_ac_mp_00.txt --m2 unlp-2023-shared-task/data/gec-fluency/valid.m2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtYe3i3DKEvM",
        "outputId": "f3129a89-9917-4071-af49-67680bafa35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading spacy resources...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_sm==2.3.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 11.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from en_core_web_sm==2.3.1) (2.3.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.9)\n",
            "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.6)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (45.2.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.10.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.30.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.65.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.8/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2023.5.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4)\n",
            "Building wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047105 sha256=7e1f1d45995d8203867813f43868af1845740d380fabe414f9c3121bc6c052e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/4d/f7/563214122be1540b5f9197b52cb3ddb9c4a8070808b22d5a84\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-2.3.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.8/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Tokenizing submission...\n",
            "2023-05-15 16:18:44 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 107MB/s]         \n",
            "2023-05-15 16:18:44 WARNING: Language uk package default expects mwt, which has been added\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-uk/resolve/v1.4.1/models/tokenize/iu.pt: 100% 646k/646k [00:00<00:00, 12.2MB/s]\n",
            "Downloading https://huggingface.co/stanfordnlp/stanza-uk/resolve/v1.4.1/models/mwt/iu.pt: 100% 552k/552k [00:00<00:00, 9.07MB/s]\n",
            "2023-05-15 16:18:45 INFO: Loading these models for language: uk (Ukrainian):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | iu      |\n",
            "| mwt       | iu      |\n",
            "=======================\n",
            "\n",
            "2023-05-15 16:18:45 INFO: Use device: cpu\n",
            "2023-05-15 16:18:45 INFO: Loading: tokenize\n",
            "2023-05-15 16:18:45 INFO: Loading: mwt\n",
            "2023-05-15 16:18:45 INFO: Done loading processors!\n",
            "Tokenized: /tmp/unlp.target.tok\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "Aligned submission: /tmp/unlp.target.m2\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "690\t488\t1108\t0.5857\t0.3838\t0.53\n",
            "==============================================\n",
            "\n",
            "\n",
            "============ Span-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "786\t392\t1081\t0.6672\t0.421\t0.5974\n",
            "==============================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ac=0.2, mep=0.5"
      ],
      "metadata": {
        "id": "HrkImhFDjO9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python unlp-2023-shared-task/scripts/evaluate.py drive/MyDrive/UNLP/predictions/111_ukr_roberta_base/prediction_tn_00_cs_4_ac_02_mp_05.txt --m2 unlp-2023-shared-task/data/gec-fluency/valid.m2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNPmBD2wjOjn",
        "outputId": "8cfd6f78-b606-4679-b2d0-9f3a43df0d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing submission...\n",
            "2023-05-19 16:00:28 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 92.0MB/s]        \n",
            "2023-05-19 16:00:28 WARNING: Language uk package default expects mwt, which has been added\n",
            "2023-05-19 16:00:28 INFO: Loading these models for language: uk (Ukrainian):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | iu      |\n",
            "| mwt       | iu      |\n",
            "=======================\n",
            "\n",
            "2023-05-19 16:00:28 INFO: Use device: cpu\n",
            "2023-05-19 16:00:28 INFO: Loading: tokenize\n",
            "2023-05-19 16:00:28 INFO: Loading: mwt\n",
            "2023-05-19 16:00:28 INFO: Done loading processors!\n",
            "Tokenized: /tmp/unlp.target.tok\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "Aligned submission: /tmp/unlp.target.m2\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "565\t322\t1146\t0.637\t0.3302\t0.5372\n",
            "==============================================\n",
            "\n",
            "\n",
            "============ Span-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "633\t254\t1152\t0.7136\t0.3546\t0.5935\n",
            "==============================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ac=0.2,mep=0.2"
      ],
      "metadata": {
        "id": "0kFYRJPUyfiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python unlp-2023-shared-task/scripts/evaluate.py drive/MyDrive/UNLP/predictions/111_ukr_roberta_base/prediction_tn_00_cs_4_ac_02_mp_02.txt --m2 unlp-2023-shared-task/data/gec-fluency/valid.m2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVh5976cyhPd",
        "outputId": "fc34ee9a-a456-4ed8-d76b-844fd011d08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing submission...\n",
            "2023-05-19 16:15:01 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 56.7MB/s]        \n",
            "2023-05-19 16:15:01 WARNING: Language uk package default expects mwt, which has been added\n",
            "2023-05-19 16:15:01 INFO: Loading these models for language: uk (Ukrainian):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | iu      |\n",
            "| mwt       | iu      |\n",
            "=======================\n",
            "\n",
            "2023-05-19 16:15:01 INFO: Use device: cpu\n",
            "2023-05-19 16:15:01 INFO: Loading: tokenize\n",
            "2023-05-19 16:15:01 INFO: Loading: mwt\n",
            "2023-05-19 16:15:01 INFO: Done loading processors!\n",
            "Tokenized: /tmp/unlp.target.tok\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "Aligned submission: /tmp/unlp.target.m2\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "634\t388\t1133\t0.6204\t0.3588\t0.5414\n",
            "==============================================\n",
            "\n",
            "\n",
            "============ Span-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "716\t306\t1130\t0.7006\t0.3879\t0.6033\n",
            "==============================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --cold_steps_count 2 --tn_prob 1 --special_tokens_fix 1 --target_vocab_size 5000"
      ],
      "metadata": {
        "id": "vmjHThNfjZ4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ac=0, mp=0"
      ],
      "metadata": {
        "id": "gSG-RMk4jeYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation lready exists"
      ],
      "metadata": {
        "id": "TkuTldcajd0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ac=0.2, mep=0.5"
      ],
      "metadata": {
        "id": "aHTUTlsGjiWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python unlp-2023-shared-task/scripts/evaluate.py drive/MyDrive/UNLP/predictions/111_ukr_roberta_base/prediction_ac_02_mep_05.txt --m2 unlp-2023-shared-task/data/gec-fluency/valid.m2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yay6kWn8jj3X",
        "outputId": "68554ede-003d-4e90-82d8-0fdf1371b2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing submission...\n",
            "2023-05-19 16:01:02 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 126MB/s]         \n",
            "2023-05-19 16:01:02 WARNING: Language uk package default expects mwt, which has been added\n",
            "2023-05-19 16:01:02 INFO: Loading these models for language: uk (Ukrainian):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | iu      |\n",
            "| mwt       | iu      |\n",
            "=======================\n",
            "\n",
            "2023-05-19 16:01:02 INFO: Use device: cpu\n",
            "2023-05-19 16:01:02 INFO: Loading: tokenize\n",
            "2023-05-19 16:01:02 INFO: Loading: mwt\n",
            "2023-05-19 16:01:02 INFO: Done loading processors!\n",
            "Tokenized: /tmp/unlp.target.tok\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "Aligned submission: /tmp/unlp.target.m2\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "445\t160\t1225\t0.7355\t0.2665\t0.544\n",
            "==============================================\n",
            "\n",
            "\n",
            "============ Span-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "488\t117\t1213\t0.8066\t0.2869\t0.5921\n",
            "==============================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ac=0.2, mep=0.2"
      ],
      "metadata": {
        "id": "uyBoAEQryRAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python unlp-2023-shared-task/scripts/evaluate.py drive/MyDrive/UNLP/predictions/111_ukr_roberta_base/prediction_ac_02_mep_02.txt --m2 unlp-2023-shared-task/data/gec-fluency/valid.m2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R-ZGQX9ySn7",
        "outputId": "e0105a51-7b50-4e65-fc1a-b316111af5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing submission...\n",
            "2023-05-19 16:13:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 84.0MB/s]        \n",
            "2023-05-19 16:13:26 WARNING: Language uk package default expects mwt, which has been added\n",
            "2023-05-19 16:13:26 INFO: Loading these models for language: uk (Ukrainian):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | iu      |\n",
            "| mwt       | iu      |\n",
            "=======================\n",
            "\n",
            "2023-05-19 16:13:26 INFO: Use device: cpu\n",
            "2023-05-19 16:13:26 INFO: Loading: tokenize\n",
            "2023-05-19 16:13:26 INFO: Loading: mwt\n",
            "2023-05-19 16:13:26 INFO: Done loading processors!\n",
            "Tokenized: /tmp/unlp.target.tok\n",
            "Loading resources...\n",
            "Processing parallel files...\n",
            "Aligned submission: /tmp/unlp.target.m2\n",
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "504\t209\t1191\t0.7069\t0.2973\t0.5542\n",
            "==============================================\n",
            "\n",
            "\n",
            "============ Span-Based Detection ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "559\t154\t1174\t0.784\t0.3226\t0.6096\n",
            "==============================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}